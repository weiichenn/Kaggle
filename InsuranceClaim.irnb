{
  "cells": [
    {
      "metadata": {
        "_uuid": "753c257ba3921ff5c430e4cca350ef0937ef27f2",
        "_cell_guid": "92858133-491d-465c-a09d-8ab3902a7d6e"
      },
      "cell_type": "markdown",
      "source": "# Introduction\n\nThis is an extensive Exploratory Data Analysis for the Porto Seguroâ€™s Safe Driver Prediction competition within the R environment of the tidyverse and ggplot2. We will visualise all the different data features, their relation to the *target* variable, explore multi-parameter interactions, and perform feature engineering. This is a standard supervised classification task.\n\nThe aim of this challenge is to predict the probability whether a driver will make an insurance claim, with the purpose of providing a fairer insurance cost on the basis of individual driving habits. It is sponsored by Porto Seguro- a major car and home insurance company in Brazil.\n\nThe data comes in the traditional Kaggle form of one training and test file each: `../input/train.csv` & `../input/test.csv`. Each row corresponds to a specific policy holder and the columns describe their features. The target variable is conveniently named *target* here and it indicates whether this policy holder made an insurance claim in the past. "
    },
    {
      "metadata": {
        "_uuid": "7b72a80cb1b48ade86252d5569c56bfe1ddee847",
        "_cell_guid": "145e3677-f37d-4a87-bddb-0f7aad6d5365"
      },
      "cell_type": "markdown",
      "source": "# Preparations"
    },
    {
      "metadata": {
        "_uuid": "7aed800556d86afac625a0c68f976e172668dc2d",
        "_cell_guid": "92ce55f4-c81b-48ce-bb2d-52408faa0f22",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# general data visualization\nlibrary('ggplot2') \nlibrary('scales') \nlibrary('grid') \nlibrary('ggthemes') \nlibrary('gridExtra') \nlibrary('RColorBrewer') \nlibrary('corrplot') \n\n# general data manipulation\nlibrary('dplyr') \nlibrary('readr') \nlibrary('data.table') \nlibrary('tibble') \nlibrary('tidyr') \nlibrary('stringr') \nlibrary('forcats') \nlibrary('rlang') \n\n# modelling\nlibrary('xgboost') \nlibrary('caret') \nlibrary('MLmetrics')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "50954a4d2dd4d100727668880e822054aecd5d76",
        "_cell_guid": "45e1f504-a2e5-4b84-bfd6-9006fed23517",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# function to extract binomial confidence levels\nget_binCI <- function(x,n) as.list(setNames(binom.test(x,n)$conf.int, c(\"lwr\", \"upr\")))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "666a731ddb7a5a1d6b3d09ad2ad1df5463c51650",
        "_cell_guid": "362db402-5756-4f45-bcc2-613b4d05dcf4"
      },
      "cell_type": "markdown",
      "source": "## Load data"
    },
    {
      "metadata": {
        "_uuid": "afc6203e4fde445c5a6c734fa345e8f1fc4548ed",
        "_cell_guid": "3bc3a6c6-af33-4ed8-b223-f4567826b3d1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train <- as.tibble(fread('../input/train.csv', na.strings=c(\"-1\",\"-1.0\")))\ntest <- as.tibble(fread('../input/test.csv', na.strings=c(\"-1\",\"-1.0\")))\nsample_submit <- as.tibble(fread('../input/sample_submission.csv'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0b0d7589ef05aa71a0f26df6cbede439cbe02ad"
      },
      "cell_type": "code",
      "source": "summary(train)\nglimpse(train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a1eefd4e5cf42f83c11fd68551369a9200d9dd37",
        "_cell_guid": "48583533-02c3-4f13-940b-4b06c471afab"
      },
      "cell_type": "markdown",
      "source": "We find:\n\n- There are lots of features here. In total, our *training* data has 59 variables, including *id* and *target*. In some of them we already see a number of NAs.\n\n- The data description mentions that the names of the features indicate whether they are binary (*bin*) or categorical (*cat*) variables. Everything else is continuous or ordinal.\n\n- We have already been told by Adriano Moala that the names of the variables indicate certain properties: *Ind\" is related to individual or driver, \"reg\" is related to region, \"car\" is related to car itself and \"calc\" is an calculated feature.' Here we will refer to these properties as groups.   "
    },
    {
      "metadata": {
        "_uuid": "fe7b2c8cadc27ebf2b2df6cff2ebe64bad059568",
        "_cell_guid": "0a6fea37-3be7-414c-839b-85341a813b3d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "summary(test)\nglimpse(test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da83c061153c14ece7e0f71b9375aa13c63376d3"
      },
      "cell_type": "code",
      "source": "sum(is.na(train))\nsum(is.na(test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6a0335221f29b465d32a9f48fdc530b041db8428",
        "_cell_guid": "8e246606-6519-4dbf-8555-027f93ec3e37"
      },
      "cell_type": "markdown",
      "source": "We will turn the categorical features into factors and the binary ones into logical values. For the *target* variable we choose a factor format.\n\n"
    },
    {
      "metadata": {
        "_uuid": "f5e31163d965c28499f53b7052af3dd4c2979f1e",
        "_cell_guid": "fc7cc070-2dc0-4cfb-8f20-bcdb11a701e2",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train <- train %>%\n  mutate_at(vars(ends_with(\"cat\")), funs(factor)) %>%\n  mutate_at(vars(ends_with(\"bin\")), funs(as.logical)) %>%\n  mutate(target = as.factor(target))\n\ntest <- test %>%\n  mutate_at(vars(ends_with(\"cat\")), funs(factor)) %>%\n  mutate_at(vars(ends_with(\"bin\")), funs(as.logical))\n\ncombine <- bind_rows(train %>% mutate(dset = \"train\"), \n                     test %>% mutate(dset = \"test\",\n                                     target = NA))\n\ncombine <- combine %>% mutate(dset = factor(dset))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "167e2b9ac8f6b163fbc301eb1e6d37962a719621",
        "_cell_guid": "3e963d8e-be1e-49a1-bf60-11a0ddd6cde3"
      },
      "cell_type": "markdown",
      "source": "# Individual feature visualizations\n\nWe start our exploration with overview distribution plots for the various features. In order to make this visualization more comprehensive, we will create layouts for the specific groups of features. For the sake of readability we divide each group into multiple parts."
    },
    {
      "metadata": {
        "_uuid": "0b491dac4ff6ce66b47d09f54bdd02b3216e7395",
        "_cell_guid": "0f64d03b-e86a-4851-a36f-007bf12f0106",
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Binary features\n\np1 <- train %>%\n  ggplot(aes(ps_ind_06_bin, fill = ps_ind_06_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np2 <- train %>%\n  ggplot(aes(ps_ind_07_bin, fill = ps_ind_07_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np3 <- train %>%\n  ggplot(aes(ps_ind_08_bin, fill = ps_ind_08_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np4 <- train %>%\n  ggplot(aes(ps_ind_09_bin, fill = ps_ind_09_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np5 <- train %>%\n  ggplot(aes(ps_ind_10_bin, fill = ps_ind_10_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np6 <- train %>%\n  ggplot(aes(ps_ind_11_bin, fill = ps_ind_11_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np7 <- train %>%\n  ggplot(aes(ps_ind_12_bin, fill = ps_ind_12_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np8 <- train %>%\n  ggplot(aes(ps_ind_13_bin, fill = ps_ind_13_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\ngrid.arrange(p1, p2, p3, p4, p5, p6, p7, p8)\n\n\np9 <- train %>%\n  ggplot(aes(ps_ind_16_bin, fill = ps_ind_16_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np10 <- train %>%\n  ggplot(aes(ps_ind_17_bin, fill = ps_ind_17_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np11 <- train %>%\n  ggplot(aes(ps_ind_18_bin, fill = ps_ind_18_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np12 <- train %>%\n  ggplot(aes(ps_calc_15_bin, fill = ps_calc_15_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np13 <- train %>%\n  ggplot(aes(ps_calc_16_bin, fill = ps_calc_16_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np14 <- train %>%\n  ggplot(aes(ps_calc_17_bin, fill = ps_calc_17_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np15 <- train %>%\n  ggplot(aes(ps_calc_18_bin, fill = ps_calc_18_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np16 <- train %>%\n  ggplot(aes(ps_calc_19_bin, fill = ps_calc_19_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np17 <- train %>%\n  ggplot(aes(ps_calc_20_bin, fill = ps_calc_20_bin)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\ngrid.arrange(p9, p10, p11, p12, p13, p14, p15, p16, p17)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1ff041fe11b102b694f8118dde29b752a9287177"
      },
      "cell_type": "markdown",
      "source": "We find that some of the binary features are very unbalanced; with \"FALSE\" accounting for the vast majority of cases. This is particularly true for the *ps\\_ind* sequence from \"10\" to \"13\". In this particular set of binary features we have more of a balance between \"TRUE\" and \"FALSE\". For the three features *ps\\_ind\\_16\\_bin*, *ps\\_calc\\_16\\_bin*, and *ps\\_calc\\_17\\_bin* we find that the \"TRUE\" values are in fact dominating.\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa0826bb6176c51780ad89cc3a0973494d4593c7"
      },
      "cell_type": "code",
      "source": "## Categorical features\n\np1 <- train %>%\n  ggplot(aes(ps_ind_02_cat, fill = ps_ind_02_cat)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\np2 <- train %>%\n  ggplot(aes(ps_ind_04_cat, fill = ps_ind_04_cat)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\np3 <- train %>%\n  ggplot(aes(ps_ind_05_cat, fill = ps_ind_05_cat)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\np4 <- train %>%\n  ggplot(aes(ps_car_01_cat, fill = ps_car_01_cat)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\np5 <- train %>%\n  ggplot(aes(ps_car_02_cat, fill = ps_car_02_cat)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\np6 <- train %>%\n  ggplot(aes(ps_car_03_cat, fill = ps_car_03_cat)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\ngrid.arrange(p1, p2, p3, p4, p5, p6)\n\n\np7 <- train %>%\n  ggplot(aes(ps_car_04_cat, fill = ps_car_04_cat)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\np8 <- train %>%\n  ggplot(aes(ps_car_05_cat, fill = ps_car_05_cat)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\np9 <- train %>%\n  ggplot(aes(ps_car_06_cat, fill = ps_car_06_cat)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\np10 <- train %>%\n  ggplot(aes(ps_car_07_cat, fill = ps_car_07_cat)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\np11 <- train %>%\n  ggplot(aes(ps_car_08_cat, fill = ps_car_08_cat)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\np12 <- train %>%\n  ggplot(aes(ps_car_09_cat, fill = ps_car_09_cat)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\np13 <- train %>%\n  ggplot(aes(ps_car_10_cat, fill = ps_car_10_cat)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\np14 <- train %>%\n  ggplot(aes(ps_car_11_cat, fill = ps_car_11_cat)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\ngrid.arrange(p7, p8, p9, p10, p11, p12, p13, p14)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f4092ce72296679ddc48f43e5a481a233ced9d7b"
      },
      "cell_type": "markdown",
      "source": "We find that some categorical features have only very few levels, down to 2 levels (+ NA) for three of them. In others we have up to 11 levels, some of which are clearly dominating the (logarithmic) plots. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a50c9216336477a9a1c6e05a078cc7b1f8ace694"
      },
      "cell_type": "code",
      "source": "## Integer features\n\np1 <- train %>%\n  mutate(ps_ind_01 = as.factor(ps_ind_01)) %>%\n  ggplot(aes(ps_ind_01, fill = ps_ind_01)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np2 <- train %>%\n  mutate(ps_ind_03 = as.factor(ps_ind_03)) %>%\n  ggplot(aes(ps_ind_03, fill = ps_ind_03)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np3 <- train %>%\n  mutate(ps_ind_14 = as.factor(ps_ind_14)) %>%\n  ggplot(aes(ps_ind_14, fill = ps_ind_14)) +\n  geom_bar() +\n  scale_y_log10() +\n  theme(legend.position = \"none\")\n\np4 <- train %>%\n  mutate(ps_ind_15 = as.factor(ps_ind_15)) %>%\n  ggplot(aes(ps_ind_15, fill = ps_ind_15)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np5 <- train %>%\n  mutate(ps_car_11 = as.factor(ps_car_11)) %>%\n  ggplot(aes(ps_car_11, fill = ps_car_11)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\ngrid.arrange(p1, p2, p3, p4, p5)\n\n\np6 <- train %>%\n  mutate(ps_calc_04 = as.factor(ps_calc_04)) %>%\n  ggplot(aes(ps_calc_04, fill = ps_calc_04)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np7 <- train %>%\n  mutate(ps_calc_05 = as.factor(ps_calc_05)) %>%\n  ggplot(aes(ps_calc_05, fill = ps_calc_05)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np8 <- train %>%\n  mutate(ps_calc_06 = as.factor(ps_calc_06)) %>%\n  ggplot(aes(ps_calc_06, fill = ps_calc_06)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np9 <- train %>%\n  mutate(ps_calc_07 = as.factor(ps_calc_07)) %>%\n  ggplot(aes(ps_calc_07, fill = ps_calc_07)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np10 <- train %>%\n  mutate(ps_calc_08 = as.factor(ps_calc_08)) %>%\n  ggplot(aes(ps_calc_08, fill = ps_calc_08)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np11 <- train %>%\n  mutate(ps_calc_09 = as.factor(ps_calc_09)) %>%\n  ggplot(aes(ps_calc_09, fill = ps_calc_09)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np12 <- train %>%\n  ggplot(aes(ps_calc_10, fill = ps_calc_10)) +\n  geom_histogram(fill = \"blue\", binwidth = 1) +\n  theme(legend.position = \"none\")\n\np13 <- train %>%\n  ggplot(aes(ps_calc_11, fill = ps_calc_11)) +\n  geom_histogram(fill = \"blue\", binwidth = 1) +\n  theme(legend.position = \"none\")\n\np14 <- train %>%\n  mutate(ps_calc_12 = as.factor(ps_calc_12)) %>%\n  ggplot(aes(ps_calc_12, fill = ps_calc_12)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np15 <- train %>%\n  mutate(ps_calc_13 = as.factor(ps_calc_13)) %>%\n  ggplot(aes(ps_calc_13, fill = ps_calc_13)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n\np16 <- train %>%\n  ggplot(aes(ps_calc_14, fill = ps_calc_14)) +\n  geom_histogram(fill = \"blue\", binwidth = 1) +\n  theme(legend.position = \"none\")\n\ngrid.arrange(p6, p7, p8, p9, p10, p11, p12, p13, p14, p15, p16)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b995ced78fdd27b018a0cdbcfbb54d2a2dea09ec"
      },
      "cell_type": "markdown",
      "source": "We find that again there are large differences in frequencies, in particular for *ps\\_ind\\_14* and *ps\\_car\\_11* where \"0\" and \"3\" are the dominating values, respectively. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78891df9b5924d12ac6025500e9ab5a9ac764b34"
      },
      "cell_type": "code",
      "source": "## Float features\n\np1 <- train %>%\n  ggplot(aes(ps_reg_01, fill = ps_reg_01)) +\n  geom_histogram(fill = \"dark green\", binwidth = 0.1) +\n  theme(legend.position = \"none\")\n\np2 <- train %>%\n  ggplot(aes(ps_reg_02, fill = ps_reg_02)) +\n  geom_histogram(fill = \"dark green\", binwidth = 0.1) +\n  theme(legend.position = \"none\")\n\np3 <- train %>%\n  ggplot(aes(ps_reg_03, fill = ps_reg_03)) +\n  geom_histogram(fill = \"dark green\", binwidth = 0.1) +\n  theme(legend.position = \"none\")\n\np4 <- train %>%\n  ggplot(aes(ps_calc_01, fill = ps_calc_01)) +\n  geom_histogram(fill = \"blue\", binwidth = 0.1) +\n  theme(legend.position = \"none\")\n\np5 <- train %>%\n  ggplot(aes(ps_calc_02, fill = ps_calc_02)) +\n  geom_histogram(fill = \"blue\", binwidth = 0.1) +\n  theme(legend.position = \"none\")\n\np6 <- train %>%\n  ggplot(aes(ps_calc_03, fill = ps_calc_03)) +\n  geom_histogram(fill = \"blue\", binwidth = 0.1) +\n  theme(legend.position = \"none\")\n\ngrid.arrange(p1, p2, p3, p4, p5, p6)\n\n\np7 <- train %>%\n  ggplot(aes(ps_car_12, fill = ps_car_12)) +\n  geom_histogram(fill = \"red\", binwidth = 0.05) +\n  theme(legend.position = \"none\")\n\np8 <- train %>%\n  ggplot(aes(ps_car_13, fill = ps_car_13)) +\n  geom_histogram(fill = \"red\", binwidth = 0.1) +\n  theme(legend.position = \"none\")\n\np9 <- train %>%\n  ggplot(aes(ps_car_14, fill = ps_car_14)) +\n  geom_histogram(fill = \"red\", binwidth = 0.01) +\n  theme(legend.position = \"none\")\n\np10 <- train %>%\n  ggplot(aes(ps_car_15, fill = ps_car_15)) +\n  geom_histogram(fill = \"red\", binwidth = 0.1) +\n  theme(legend.position = \"none\")\n\ngrid.arrange(p7, p8, p9, p10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a2f8d33002a27b21e685c180a13403a8bbc5f02c"
      },
      "cell_type": "markdown",
      "source": "We find that while the (green) \"reg\" features show distributions that are clearly skewed toward a prominent peak, the (blue) \"calc\" features appear to be pretty uniformly distributed.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5795cd0b5ecabd2e2eb86919bc5cb8fdaf090a24"
      },
      "cell_type": "code",
      "source": "## Target variable\n\ntrain %>%\n  ggplot(aes(target, fill = target)) +\n  geom_bar() +\n  theme(legend.position = \"none\")\n  \ntrain %>%\n  group_by(target) %>%\n  summarise(percentage = n()/nrow(train)*100)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d86feec14c9375d260a2c7251156eb013185edb8"
      },
      "cell_type": "markdown",
      "source": "With less than 4% of policy holders filing a claim the problem is heavily imbalanced."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e744f2b251a7c7af16f43e36ea22958e6c6339c2"
      },
      "cell_type": "code",
      "source": "sum(is.na(train))/(nrow(train)*ncol(train))*100\nsum(is.na(test))/(nrow(test)*ncol(test))*100",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a28a5d28e000f03afeca809b4ea23851ea34b6ca"
      },
      "cell_type": "markdown",
      "source": "# Multi-feature comparisons\n\n## Correlation overview\n\nWe begin with a correlation matrix plot as a first comprehensive overview of our multi-parameter space. \n\nWhat we will see here is the correlation coefficients for each combination of two features. In simplest terms: this shows whether two features are connected so that one changes with a predictable trend if you change the other. The closer this coefficient is to zero the weaker is the correlation. Both 1 and -1 are the ideal cases of perfect correlation and anti-correlation.\n\nNote that for the purpose of this plot we will recode our binary features and categorical as integers. All rows with NAs are excluded."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ac8ac80996c09ed2d1056895f188841839ac3b3c"
      },
      "cell_type": "code",
      "source": "train %>%\n  select(-starts_with(\"ps_calc\"), -ps_ind_10_bin, -ps_ind_11_bin, -ps_car_10_cat, -id) %>%\n  mutate_at(vars(ends_with(\"cat\")), funs(as.integer)) %>%\n  mutate_at(vars(ends_with(\"bin\")), funs(as.integer)) %>%\n  mutate(target = as.integer(target)) %>%\n  cor(use=\"complete.obs\", method = \"spearman\") %>%\n  corrplot(type=\"lower\", tl.col = \"black\",  diag=FALSE)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fd562a1ee4a88f4a0890aea70b5e2a0e26fce173"
      },
      "cell_type": "code",
      "source": "train %>%\n  select(ps_ind_12_bin, ps_ind_14, ps_ind_16_bin, ps_ind_17_bin, ps_ind_18_bin, ps_reg_02,\n         ps_reg_03, ps_car_12, ps_car_13, ps_car_14, ps_car_15, ps_car_02_cat, ps_car_04_cat) %>%\n  mutate_at(vars(ends_with(\"cat\")), funs(as.integer)) %>%\n  mutate_at(vars(ends_with(\"bin\")), funs(as.integer)) %>%\n  cor(use=\"complete.obs\", method = \"spearman\") %>%\n  corrplot(type=\"lower\", tl.col = \"black\",  diag=FALSE, method = \"number\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "19fd3e77f258b34cb0442f3a76f089032b611017",
        "_cell_guid": "c31cdf14-a136-4635-a981-1667b604a028"
      },
      "cell_type": "markdown",
      "source": "We find:\n\n- There is a very strong correlation between *ps\\_ind\\_12\\_bin* and *ps\\_ind\\_14*, which is an ordinal integer feature with 5 levels. Other correlations that exist are weaker but still notable.\n\n- The correlation between the \"reg\" and \"car\" features, respectively, shows how continuous variables are related. In particular *ps\\_car\\_14*, which showed only a small effect in the individual plots, might be interesting here. \n"
    },
    {
      "metadata": {
        "_uuid": "2f43b8f3c6e076fd38194b084f55937495dccb62"
      },
      "cell_type": "markdown",
      "source": "### Pairwise relationships\n\n\nWe begin with a layout of plots that examine the one-to-one relations for all pairings with an (absolute value) *correlation coefficient above 0.5*"
    },
    {
      "metadata": {
        "_uuid": "17edda2350cbb6541a76f0ce4f104fbfe94a1dff",
        "_cell_guid": "1f1aac96-81ef-457c-b26f-a27815d592cc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "p1 <- train %>%\n  ggplot(aes(ps_ind_14, fill = ps_ind_12_bin)) +\n  geom_bar(position = \"fill\")\n\np2 <- train %>%\n  ggplot(aes(ps_ind_16_bin, ps_ind_18_bin)) +\n  geom_count(color = \"orange\")\n\np3 <- train %>%\n  ggplot(aes(ps_ind_16_bin, ps_ind_17_bin)) +\n  geom_count(color = \"orange\")\n\np4 <- train %>%\n  ggplot(aes(ps_reg_02, ps_reg_03)) +\n  geom_point() +\n  geom_smooth(method = 'gam', color = \"dark green\")\n  \np5 <- train %>%\n  ggplot(aes(ps_car_12, ps_car_13)) +\n  geom_point() +\n  geom_smooth(method = 'gam', color = \"red\")\n\np6 <- train %>%\n  ggplot(aes(ps_car_12, ps_car_14)) +\n  geom_point() +\n  geom_smooth(method = 'gam', color = \"red\")\n\ngrid.arrange(p1, p2, p3, p4, p5, p6)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1c58be4ce02ac1aae21373a998ba3dc3c097b299",
        "_cell_guid": "0e2ba356-e4f5-4a9e-90f6-246bcdb05521"
      },
      "cell_type": "markdown",
      "source": "We find:\n\n- The barplot of *ps\\_ind\\_14* demonstrates the strong correlation with *ps\\_ind\\_12\\_bin* that stems mostly from the practically exclusive association of `ps_ind_14 == 0` with `ps_ind_12_bin == FALSE` and `ps_ind_14 == 1` with `ps_ind_12_bin == TRUE`. And even the *ps\\_\\ind\\_14* values in between show a gradually increasing fraction of `ps_ind_12_bin == TRUE`.\n\n- We study the *binary* feature relations using count plots, where the size of the dots correspond to the number of cases. For *ps\\_ind\\_16_bin* vs *ps\\_ind\\_17_bin* and *ps\\_ind\\_18_bin* we see that the predominant associations are `ps_ind_16_bin == TRUE` with `ps_ind_17_bin == FALSE` and `ps_ind_18_bin == FALSE`, respectively. There are no cases in these two relations were both features are *TRUE*.\n\n- For the floating point features we use a scatter plot together with a simple smoothing model to visualise their relationships. In case of *ps\\_reg\\_02* we clearly see its distinct values. Here the linear relation is formally present but doesn't look too convincing, considering the largely overlapping ranges of *ps\\_reg\\_03*.\n\n- In contrast, the two *ps\\_car* feature plots show a decent correlation that might even be underestimated by the coefficients due to the presence of obvious outliers."
    },
    {
      "metadata": {
        "_uuid": "e02c8f1f24ec30644970c06fed72e07a5b0036f0",
        "_cell_guid": "766a7962-227f-4813-b25f-cb077b93fef8"
      },
      "cell_type": "markdown",
      "source": "# Feature engineering\n\nOnce we have sufficiently explored the connection within the existing features our next step will be to use these insights to build new features.\n\nWe work on the combined data frame to make sure that our *train* and *test* data are treated consistently."
    },
    {
      "metadata": {
        "_uuid": "62b9cc99084a784a872e59f102f23134734c727c",
        "_cell_guid": "1ba64a92-216a-4f50-8432-aa789229e5c9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "nano <- combine %>%\n  is.na() %>%\n  rowSums() %>%\n  as.integer()\n\nbin_ind <- combine %>% select(ends_with(\"bin\")) %>%\n  select(starts_with(\"ps_ind\")) %>%\n  rowSums() %>%\n  as.integer()\n\nbin_calc <- combine %>% select(ends_with(\"bin\")) %>%\n  select(starts_with(\"ps_calc\")) %>%\n  rowSums() %>%\n  as.integer()\n\nbins <- combine %>%\n  select(ends_with(\"bin\")) %>%\n  select(starts_with(\"ps_ind\")) %>%\n  mutate_all(funs(as.integer))\n\nref_bin <- bins %>% summarise_all(median, na.rm = TRUE)\nref_bin <- ref_bin[rep(1,nrow(combine)),]\ndiff_ind <- rowSums(abs(bins - ref_bin))\n\n\nbins <- combine %>%\n  select(ends_with(\"bin\")) %>%\n  select(starts_with(\"ps_calc\")) %>%\n  mutate_all(funs(as.integer))\n\nref_bin <- bins %>% summarise_all(median, na.rm = TRUE)\nref_bin <- ref_bin[rep(1,nrow(combine)),]\ndiff_calc <- rowSums(abs(bins - ref_bin))\n\n\ncombine <- combine %>%\n  mutate(nano = nano,\n         bin_ind = bin_ind,\n         bin_calc = bin_calc,\n         diff_ind = diff_ind,\n         diff_calc = diff_calc)\n\ntrain <- combine %>%\n  filter(dset == \"train\")\n\ntest <- combine %>%\n  filter(dset == \"test\")\n\n\np1 <- combine %>%\n  ggplot(aes(ps_reg_01, fill = dset)) +\n  geom_density(alpha = 0.5, bw = 0.05) +\n  theme(legend.position = \"none\")\n\np2 <- combine %>%\n  ggplot(aes(ps_reg_02, fill = dset)) +\n  geom_density(alpha = 0.5, bw = 0.05) +\n  theme(legend.position = \"none\")\n\np3 <- combine %>%\n  ggplot(aes(ps_reg_03, fill = dset)) +\n  geom_density(alpha = 0.5, bw = 0.05) +\n  theme(legend.position = \"none\")\n\np4 <- combine %>%\n  ggplot(aes(ps_calc_01, fill = dset)) +\n  geom_density(alpha = 0.5, bw = 0.05) +\n  theme(legend.position = \"none\")\n\np5 <- combine %>%\n  ggplot(aes(ps_calc_02, fill = dset)) +\n  geom_density(alpha = 0.5, bw = 0.05) +\n  theme(legend.position = \"none\")\n\np6 <- combine %>%\n  ggplot(aes(ps_calc_03, fill = dset)) +\n  geom_density(alpha = 0.5, bw = 0.05)\n\ngrid.arrange(p1, p2, p3, p4, p5, p6)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "46d0c1cd9221247388c41453ee393f0564550c3d",
        "_cell_guid": "9b9c84ce-fb20-4788-96ee-c2e1dd9f8910"
      },
      "cell_type": "markdown",
      "source": "## Feature selection, evaluation metric, and validation split"
    },
    {
      "metadata": {
        "_uuid": "77e559bd8fd988de8868c3a2caa8f1311b7ffe8c",
        "_cell_guid": "6896589e-e23d-4620-aa4e-32dda1556aca"
      },
      "cell_type": "markdown",
      "source": "The model evaluation metric here is the Normalized Gini Function. It has the advantage that only the relative order of the predicted probabilities is measured, not their absolute value. Thereby, your prediction is successful as long as it assigns low probabilities to true \"no claims\" observations and high probabilities to true \"claims\". The \"normalized\" property means that probabilities are measured in the interval [0,1]. The metric then orders your predictions by probability and the more \"claims\" (or \"no claims\") are among the high (or low) probabilities the better the result.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8839b6f0bcb43d721c48110edd50f487caec1bb4"
      },
      "cell_type": "code",
      "source": "# Feature formatting\n\ncombine <- combine %>%\n  mutate_at(vars(ends_with(\"cat\")), funs(as.integer)) %>%\n  mutate_at(vars(ends_with(\"bin\")), funs(as.integer)) %>%\n  mutate(target = as.integer(levels(target))[target])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9d1137d67078e1bcb218bdf8858f42089a978a7"
      },
      "cell_type": "code",
      "source": "# predictor features\n\nind_cols <- c(\"ps_ind_01\",\"ps_ind_02_cat\",\"ps_ind_03\",\"ps_ind_04_cat\",\"ps_ind_05_cat\",\n              \"ps_ind_06_bin\",\"ps_ind_07_bin\",\"ps_ind_08_bin\",\"ps_ind_09_bin\",\"ps_ind_10_bin\",\n              \"ps_ind_11_bin\",\"ps_ind_12_bin\",\"ps_ind_13_bin\",\"ps_ind_14\",\"ps_ind_15\",\n              \"ps_ind_16_bin\",\"ps_ind_17_bin\",\"ps_ind_18_bin\")\n\nreg_cols <- c(\"ps_reg_01\",\"ps_reg_02\",\"ps_reg_03\")\n\ncar_cols <- c(\"ps_car_01_cat\",\"ps_car_02_cat\",\"ps_car_03_cat\",\"ps_car_04_cat\",\"ps_car_05_cat\",\n              \"ps_car_06_cat\",\"ps_car_07_cat\",\"ps_car_08_cat\",\"ps_car_09_cat\",\"ps_car_10_cat\",\n              \"ps_car_11_cat\",\"ps_car_11\",\"ps_car_12\",\"ps_car_13\",\"ps_car_14\",\"ps_car_15\")\n\ncalc_cols <- c(\"ps_calc_01\",\"ps_calc_02\",\"ps_calc_03\",\"ps_calc_04\",\"ps_calc_05\",\n               \"ps_calc_06\",\"ps_calc_07\",\"ps_calc_08\",\"ps_calc_09\",\"ps_calc_10\",\n               \"ps_calc_11\",\"ps_calc_12\",\"ps_calc_13\",\"ps_calc_14\",\"ps_calc_15_bin\",\n               \"ps_calc_16_bin\",\"ps_calc_17_bin\",\"ps_calc_18_bin\",\"ps_calc_19_bin\",\"ps_calc_20_bin\")\n\neng_cols <- c(\"nano\", \"bin_ind\", \"bin_calc\", \"diff_ind\", \"diff_calc\")\n\ntrain_cols <- c(ind_cols, reg_cols, car_cols, eng_cols)\n\n# target feature\n\ny_col <- c(\"target\")\n\n# identification feature\n\nid_col <- c(\"id\") \n\n# auxilliary features\n\naux_cols <- c(\"dset\")\n\n# extract test id column\n\ntest_id <- combine %>%\n  filter(dset == \"test\") %>%\n  select(!!sym(id_col))\n\n# all relevant columns for train/test\n\ncols <- c(train_cols, y_col, aux_cols)\n\ncombine <- combine %>%\n  select_(.dots = cols)\n\n\n# split train/test\n\ntrain <- combine %>%\n  filter(dset == \"train\") %>%\n  select_(.dots = str_c(\"-\",c(aux_cols)))\n\ntest <- combine %>%\n  filter(dset == \"test\") %>%\n  select_(.dots = str_c(\"-\",c(aux_cols, y_col)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5c17f7955558440b1e476eea72e391c49b8a143e",
        "_cell_guid": "7e8ceb16-f981-44fa-a790-81834953ad01",
        "trusted": true
      },
      "cell_type": "code",
      "source": "xgb_normalizedgini <- function(preds, dtrain){\n  actual <- getinfo(dtrain, \"label\")\n  score <- NormalizedGini(preds,actual)\n  return(list(metric = \"NormalizedGini\", value = score))\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3fd1bc4a65570c1e21039460ead5158bcd936e8a",
        "_cell_guid": "9c94e399-b176-41ff-a3ac-abf6bf16a83b"
      },
      "cell_type": "markdown",
      "source": "In order to assess how well our model generalises we will perform a cross-validation step and also split our training data into a *train* vs *validation* data set. Thereby, the model performance can be evaluated on a data sample that the algorithm has not seen. We split our data into 80/20 fractions:"
    },
    {
      "metadata": {
        "_uuid": "22809f6675630246b539841555d2ba3d85ae3a5c",
        "_cell_guid": "e993dfb0-a4f2-4b85-8379-67f1d32a103d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "set.seed(4321)\ntrainIndex <- createDataPartition(train$target, p = 0.8, list = FALSE, times = 1)\ntrain <- train[trainIndex,]\nvalid <- train[-trainIndex,]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "791c077d5c1a902a3b614dcb1f6c4e9b8df32bc8",
        "_cell_guid": "0371c583-d1c0-4061-98b7-02c81f87f951"
      },
      "cell_type": "markdown",
      "source": "## XGBoost parameters and fitting\n\n\nWe fit our model using *XGBoost* because it's a robust and easy to use tool. In essence, it is a decision-tree gradient boosting algorithm with a high degree of popularity here on Kaggle.  A few brief words about *gradient boosting*, as far as I understand it: Boosting is what we call the step-by-step improvement of a weak learner (like a relatively shallow decision tree of *max\\_depth* levels) by successively applying it to the results of the previous learning step (for *nrounds* times in total). *Gradient Boosting* focusses on minimising the Loss Function (according to our evaluation metric) by training the algorithm on the gradient of this function. The method of *Gradient Decent* iteratively moves into the direction of the greatest decent (i.e. most negative first derivative) of the loss function. The step sizes can vary from iteration to iteration but has a multiplicative *shrinkage factor eta in (0,1]* associated with it for additional tuning. Smaller values of eta result in a slower decent and require higher *nrounds*.\n"
    },
    {
      "metadata": {
        "_uuid": "e6e1efb28c27d0eb52d0ee6454d67d7447402295",
        "_cell_guid": "122119ef-dc0a-4d9c-a29f-1c887cc98cb1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#convert to XGB matrix\n\nfoo <- train %>% select(-target)\nbar <- valid %>% select(-target)\n\ndtrain <- xgb.DMatrix(as.matrix(foo),label = train$target)\ndvalid <- xgb.DMatrix(as.matrix(bar),label = valid$target)\ndtest <- xgb.DMatrix(as.matrix(test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "889029bbd5d09c9668c220d48ae5c54727cc1edd",
        "_cell_guid": "1f615d1c-bd4e-479a-9446-4d8b44698a7b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "xgb_params <- list(colsample_bytree = 0.7, #variables per tree \n                   subsample = 0.7, #data subset per tree \n                   booster = \"gbtree\",\n                   max_depth = 5, #tree levels\n                   eta = 0.3, #shrinkage\n                   eval_metric = xgb_normalizedgini,\n                   objective = \"reg:logistic\",\n                   seed = 4321,\n                   nthread = -1\n                   )\n\nwatchlist <- list(train=dtrain, valid=dvalid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "afbd078917fda77ba4ff32af1a7f9d558dd2778f",
        "_cell_guid": "ae325529-15d4-4e09-86e8-d736d65a5541",
        "trusted": true
      },
      "cell_type": "code",
      "source": "set.seed(1234)\nxgb_cv <- xgb.cv(xgb_params,dtrain,early_stopping_rounds = 5, nfold = 5, nrounds=50, maximize = TRUE)\n\nset.seed(4321)\ngb_dt <- xgb.train(params = xgb_params,\n                   data = dtrain,\n                   print_every_n = 5,\n                   watchlist = watchlist,\n                   nrounds = xgb_cv$best_iteration)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7ce3de82ed85c780f3899b6e475152e46d5bee23",
        "_cell_guid": "295b9b12-a138-4b33-b901-3b41126efbf3"
      },
      "cell_type": "markdown",
      "source": "## Feature importance"
    },
    {
      "metadata": {
        "_uuid": "fe64a7761ff9da2edc049af9b3988a44e2d73d53",
        "_cell_guid": "2b6dee2c-5a8e-41db-bf55-1ebf554adcd7",
        "trusted": true
      },
      "cell_type": "code",
      "source": "imp_matrix <- as.tibble(xgb.importance(feature_names = colnames(train %>% select(-target)), model = gb_dt))\n\nimp_matrix %>%\n  ggplot(aes(reorder(Feature, Gain, FUN = max), Gain, fill = Feature)) +\n  geom_col() +\n  coord_flip() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Features\", y = \"Importance\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0aea69cf25039f66eb7cce3a4a8ff0e56ca8a78a",
        "_cell_guid": "23fe69de-9d2c-404b-878c-433759cbb243"
      },
      "cell_type": "markdown",
      "source": "We find:\n\n- As pointed out in several other modelling kernels, the features *ps\\_car\\_13* and *ps\\_reg\\_03* are the most important ones for our tentative prediction model. Also note the two *ps\\_ind* features on 3rd and 4th rank.\n\n- In addition, our engineered features are not doing bad either. *diff\\_ind* is ranked the 5th most significant feature, and *nano* still has some impact, too. *diff\\_calc*, *bin\\_calc*, and *bin\\_ind* are towards the end of the list but (not only) in this competition small improvements can have a large impact on the resulting leaderboard score.\n\n- Those results are preliminary, since the model performance can certainly be optimized, but they already show that feature engineering is a promising way of improving your prediction.\n"
    },
    {
      "metadata": {
        "_uuid": "16b3c3a9e80159f5e2ac144b68e06a987b3b8a2d",
        "_cell_guid": "6a79a6d3-c3c8-44de-b7c6-68e7f55ea4b2"
      },
      "cell_type": "markdown",
      "source": "## Prediction and submission file"
    },
    {
      "metadata": {
        "_uuid": "638a9be1e054d804a1a9b6556d4a7f1843da5a04",
        "_cell_guid": "9d957b64-0dc7-48b6-b7ab-592b70a3e3a0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "pred <- test_id %>%\n  mutate(target = predict(gb_dt, dtest))\n\npred %>% write_csv('submit.csv')\n\nidentical(dim(sample_submit),dim(pred))\nglimpse(sample_submit)\nglimpse(pred)\n\nbind_cols(sample_submit, pred) %>% head(5)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.2",
      "file_extension": ".r",
      "codemirror_mode": "r"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}